{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9cb25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudaq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "cudaq.set_target(\"qpp-cpu\")  # or \"nvidia\" if you have CUDA-Q GPU build\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f970438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Quantum Function (2 qubits)\n",
    "# ---------------------\n",
    "class QuantumFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, thetas: torch.Tensor, quantum_circuit, shift: float):\n",
    "        ctx.shift = shift\n",
    "        ctx.quantum_circuit = quantum_circuit\n",
    "\n",
    "        # Run kernel → expectation values\n",
    "        exp_vals = ctx.quantum_circuit.run(thetas).reshape(-1, 1)\n",
    "        ctx.save_for_backward(thetas, exp_vals)\n",
    "        return exp_vals\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        thetas, _ = ctx.saved_tensors\n",
    "        grads = torch.zeros_like(thetas)\n",
    "        s = ctx.shift\n",
    "        qc = ctx.quantum_circuit\n",
    "\n",
    "        for j in range(thetas.shape[1]):\n",
    "            tp = thetas.clone(); tp[:, j] += s\n",
    "            tm = thetas.clone(); tm[:, j] -= s\n",
    "            exp_p = qc.run(tp).view(-1,1)\n",
    "            exp_m = qc.run(tm).view(-1,1)\n",
    "            dE = (exp_p - exp_m) / (2.0 * s)\n",
    "            grads[:, j:j+1] = dE * grad_output\n",
    "        return grads, None, None\n",
    "\n",
    "# ---------------------\n",
    "# Quantum Circuit Wrapper\n",
    "# ---------------------\n",
    "class QuantumCircuitWrapper:\n",
    "    def __init__(self, qubit_count: int, hamiltonian: cudaq.SpinOperator):\n",
    "        @cudaq.kernel\n",
    "        def kernel(qubit_count: int, thetas: np.ndarray):\n",
    "            q = cudaq.qvector(qubit_count)\n",
    "\n",
    "            # Encode qubit 0\n",
    "            ry(thetas[0], q[0])\n",
    "            rx(thetas[1], q[0])\n",
    "\n",
    "            # Encode qubit 1\n",
    "            ry(thetas[2], q[1])\n",
    "            rx(thetas[3], q[1])\n",
    "\n",
    "            # Entangle\n",
    "            cx(q[0], q[1])\n",
    "\n",
    "        self.kernel = kernel\n",
    "        self.qubit_count = qubit_count\n",
    "        self.hamiltonian = hamiltonian\n",
    "\n",
    "    def run(self, theta_vals: torch.Tensor) -> torch.Tensor:\n",
    "        theta_np = theta_vals.detach().cpu().numpy()\n",
    "        exps = []\n",
    "        for i in range(theta_np.shape[0]):\n",
    "            res = cudaq.observe(\n",
    "                self.kernel,\n",
    "                self.hamiltonian,\n",
    "                self.qubit_count,\n",
    "                np.array(theta_np[i], dtype=np.float64)\n",
    "            )\n",
    "            exps.append(res.expectation())\n",
    "        return torch.tensor(exps, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---------------------\n",
    "# Quantum Layer (2 qubits)\n",
    "# ---------------------\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, qubit_count: int, hamiltonian, shift: float):\n",
    "        super().__init__()\n",
    "        self.quantum_circuit = QuantumCircuitWrapper(qubit_count, hamiltonian)\n",
    "        self.shift = shift\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Apply custom autograd function\n",
    "        return QuantumFunction.apply(x, self.quantum_circuit, self.shift)\n",
    "\n",
    "# ---------------------\n",
    "# Residual QNN with 2-qubit quantum block\n",
    "# ---------------------\n",
    "class ResidualQNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.skip_linear = nn.Linear(128, 64)\n",
    "\n",
    "        # Quantum block (2 qubits → 4 params per sample)\n",
    "        self.fc_map = nn.Linear(64, 4)  # map classical → 4 angles\n",
    "        self.quantum = QuantumLayer(\n",
    "            qubit_count=2,\n",
    "            hamiltonian=(cudaq.spin.z(0) + cudaq.spin.z(1)) / 2.0,\n",
    "            shift=np.pi / 2\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        residual = self.skip_linear(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = x + residual\n",
    "\n",
    "        # compress 64 → 4 parameters (for 2 qubits)\n",
    "        angles = self.fc_map(x)\n",
    "        q_out = self.quantum(angles).view(-1, 1)\n",
    "\n",
    "        return self.fc_out(q_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beb21e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "#   Data (predict Sint)\n",
    "# ---------------------\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "dataset_path = \"../../data/data.csv\"\n",
    "data = load_data(dataset_path)\n",
    "\n",
    "# numeric fill\n",
    "num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "data[num_cols] = data[num_cols].fillna(data[num_cols].mean())\n",
    "\n",
    "# Features: drop ID-like and target columns\n",
    "drop_cols = ['ID', 'Unnamed: 0', 'Sint', 'e_Sint']\n",
    "feature_cols = [c for c in data.columns if c not in drop_cols]\n",
    "X = data[feature_cols].values.astype(np.float32)\n",
    "\n",
    "# Target: Sint (1D)\n",
    "y = data['Sint'].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Normalize\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(X).astype(np.float32)\n",
    "y = scaler_y.fit_transform(y).astype(np.float32)  # train on scaled y for stability\n",
    "\n",
    "# Tensors\n",
    "X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "y = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e15ce2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.630602, Std Dev: 0.000000\n",
      "Epoch 2/20, Loss: 1.628107, Std Dev: 0.001247\n",
      "Epoch 3/20, Loss: 1.571103, Std Dev: 0.027479\n",
      "Epoch 4/20, Loss: 1.506126, Std Dev: 0.050862\n",
      "Epoch 5/20, Loss: 1.428799, Std Dev: 0.076959\n",
      "Epoch 6/20, Loss: 1.326630, Std Dev: 0.109770\n",
      "Epoch 7/20, Loss: 1.213537, Std Dev: 0.146537\n",
      "Epoch 8/20, Loss: 1.120244, Std Dev: 0.179811\n",
      "Epoch 9/20, Loss: 1.075212, Std Dev: 0.202588\n",
      "Epoch 10/20, Loss: 1.085574, Std Dev: 0.212650\n",
      "Epoch 11/20, Loss: 1.103379, Std Dev: 0.215620\n",
      "Epoch 12/20, Loss: 1.098939, Std Dev: 0.216538\n",
      "Epoch 13/20, Loss: 1.081472, Std Dev: 0.217203\n",
      "Epoch 14/20, Loss: 1.064757, Std Dev: 0.217728\n",
      "Epoch 15/20, Loss: 1.049580, Std Dev: 0.218124\n",
      "Epoch 16/20, Loss: 1.039027, Std Dev: 0.218203\n",
      "Epoch 17/20, Loss: 1.036216, Std Dev: 0.217661\n",
      "Epoch 18/20, Loss: 1.035148, Std Dev: 0.216611\n",
      "Epoch 19/20, Loss: 1.032148, Std Dev: 0.215297\n",
      "Epoch 20/20, Loss: 1.029389, Std Dev: 0.213800\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#   Training\n",
    "# ---------------------\n",
    "model = ResidualQNN(input_dim=X.shape[1]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.MSELoss().to(device)\n",
    "\n",
    "epochs = 20\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(X_train)\n",
    "    loss = loss_function(y_hat, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}, Std Dev: {np.std(losses):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "814e0874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation (scaled) ---\n",
      "R^2:  0.0637\n",
      "MAE:  0.2040\n",
      "RMSE: 0.5820\n",
      "MSE:  0.3387\n",
      "\n",
      "--- Evaluation (original Sint units) ---\n",
      "R^2:  0.0637\n",
      "MAE:  2.0747 | Accuracy: 96.51%\n",
      "RMSE: 5.9184 | Accuracy: 90.06%\n",
      "MSE:  35.0277 | Accuracy: 41.15%\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "#   Evaluation\n",
    "# ---------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat_test = model(X_test)\n",
    "\n",
    "# to numpy (scaled space)\n",
    "y_test_np = y_test.detach().cpu().numpy()\n",
    "y_hat_np  = y_hat_test.detach().cpu().numpy()\n",
    "\n",
    "# Metrics (scaled)\n",
    "r2_scaled  = r2_score(y_test_np, y_hat_np)\n",
    "mae_scaled = mean_absolute_error(y_test_np, y_hat_np)\n",
    "rmse_scaled = np.sqrt(mean_squared_error(y_test_np, y_hat_np))\n",
    "mse_scaled  = mean_squared_error(y_test_np, y_hat_np)\n",
    "\n",
    "print(\"\\n--- Evaluation (scaled) ---\")\n",
    "print(f\"R^2:  {r2_scaled:.4f}\")\n",
    "print(f\"MAE:  {mae_scaled:.4f}\")\n",
    "print(f\"RMSE: {rmse_scaled:.4f}\")\n",
    "print(f\"MSE:  {mse_scaled:.4f}\")\n",
    "\n",
    "# Metrics in original units\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_np)\n",
    "y_pred_orig = scaler_y.inverse_transform(y_hat_np)\n",
    "\n",
    "r2_orig  = r2_score(y_test_orig, y_pred_orig)\n",
    "mae_orig = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "rmse_orig = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))\n",
    "mse_orig  = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "\n",
    "range_y = y_test_orig.max() - y_test_orig.min()\n",
    "mae_acc  = (1 - mae_orig / range_y) * 100 if range_y > 0 else np.nan\n",
    "rmse_acc = (1 - rmse_orig / range_y) * 100 if range_y > 0 else np.nan\n",
    "mse_acc  = (1 - mse_orig / range_y) * 100 if range_y > 0 else np.nan\n",
    "\n",
    "print(\"\\n--- Evaluation (original Sint units) ---\")\n",
    "print(f\"R^2:  {r2_orig:.4f}\")\n",
    "print(f\"MAE:  {mae_orig:.4f} | Accuracy: {mae_acc:.2f}%\")\n",
    "print(f\"RMSE: {rmse_orig:.4f} | Accuracy: {rmse_acc:.2f}%\")\n",
    "print(f\"MSE:  {mse_orig:.4f} | Accuracy: {mse_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224849b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
